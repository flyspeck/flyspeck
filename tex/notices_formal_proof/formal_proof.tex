% Formal Proof: A Mathematician's Perspective
% Author: Thomas C. Hales
% Affiliation: University of Pittsburgh
% email: hales@pitt.edu
%
% latex format

%% For the EDITOR:
%% References to Boxes are marked XX.  They must be filled in with appropriate labels.
%% The two boxes "The HOL Light System" with items 1,2,3 should be combined into a single box with the one that follows (items 4,5) and typeset to appear on a single page.  The eight rules of item 4 should be typeset into two columns with the first four appearing in the first column and the last four in the second.


% History.  File started Feb 18, 2008
% Revisions Aug 16, 2008
% Sep 1, 2008 added de Bruijn, sent to Notices
% Sep 5, fixed Harrison citation.
%% 

% Notes to myself:
% artifact (in wider use), but -> artefact = British and dictionary preferred.
%  (skipped) SML type safefy formalization after Leroy,  http://portal.acm.org/citation.cfm?id=1190215.1190245 



\documentclass{llncs}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{alltt}


% Math notation.
\def\op#1{{\hbox{#1}}} 
\def\tc{\hbox{:}}
\newcommand{\ring}[1]{\mathbb{#1}}

% Flags


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Formal Proof}
\author{Thomas C. Hales\thanks{The author's research on the Formal Foundations of Discrete Geometry has been supported by NSF grant 0503447.  He thanks Mark Adams, K. Parkinson, B. Casselman, and F. Wiedijk for helpful comments.}}
\institute{University of Pittsburgh\\
\email{hales@pitt.edu}}
\maketitle

\centerline{\it dedicated to N.G. de Bruijn}

\bigskip

{\narrower\it 

There remains but one course 
for the recovery of a sound and healthy condition --
namely, that the entire work of the understanding be commenced afresh, and the mind itself be from the very outset not left to take its own course, but
guided at every step; and the business be done
as if by machinery.  F. Bacon, 1620, Novum Organum
% preface.

}

\bigskip


%%%%%%%%%%%%%%%%%%%%%%

\bigskip

\section{Bugs}

Daily, we confront the errors of computers.  They crash,
hang, succumb to viruses, run buggy software, and harbor
spyware.  Our  tabloids report bizarre computer glitches:
the library patron who is fined \$40 trillion for an overdue
book, because a  
barcode is scanned as the size of the fine; or the 
dentist in San Diego who was delivered over 16 thousand tax forms
to his doorstep when he abbreviated ``suite'' 
in his address as ``su.''

% http://amartester.blogspot.com/2007/04/bugs-per-lines-of-code.html
  On average,
a programmer introduces 1.5 bugs per line while typing.
% The "private bug rate" of 1.5 per line refers to "Beizer 1984" referred to on page 23 of "Testing Computer software" by Kaner et al.
Most are typing errors that are spotted at once.
About one bug per hundred lines of computer code ships  to
market without detection.  Bugs are an accepted
part of programming culture.
The book that describes itself as the ``bestselling software testing
book of all time'' states that ``testers shouldn't want to verify
that a program runs correctly''~\cite{KFN}.
Another book on software
testing states ``Don't insist that every bug be fixed $\ldots$
When the programmer fixes a minor bug, he might create
a more serious one.''  Corporations may keep critical bugs
off the books to
limit legal liability.
 Only those bugs should be corrected
that affect  profit.
The tools designed to root out bugs are themselves
full of bugs. ``Indeed, test tools are often buggier than
comparable (but cheaper) development tools''
\cite{KBP}.
% Lessons Learned in Software Testing, Kaner, Bach, Pettichord 2001.
As for hardware reliability, former 
Intel President Andy Grove himself said 
``I have come to the conclusion that no microprocessor is ever
perfect; they just come closer to perfection $\ldots$''
\cite[p.221]{Mac}.
% quoted p221. MacKenzie, Mechanizing Proof.
%(at the time of the famous pentium bug) 


Bugs can be far-reaching.
The bug causing the 
explosion of the Ariane 5 rocket cost hundreds of millions
of dollars.  As long ago as 1854, Thoreau wrote that 
``by the error of some calculator
the vessel often splits upon a rock that should have reached
a friendly pier.''  % Walden, Economy, page 13.
Last year, the New York Times reported Shamir's warning that
even a small math error in a widely used computer chip could 
be exploited to defeat cryptography and would
place
``the security of the global electronic commerce system at risk
$\ldots$''~\cite{NYT}.
% Adding Math to List of Security Threats, New York Times, November 17, 2007.





\section{Mathematical Certainty}

By contrast, philosophers tell us that
mathematics consists of analytic truths,
free of all imperfection.  We prove that $1+1=2$ by
recalling the definition of $1$ as the successor of $0$,
$2$ as the successor of $1$, and then invoking twice the recursive
definition
of addition: 
  $$1+1 = 1 + S(0) = S(1 + 0) = S(1) = 2.$$

If only all proofs were so simple.  
Mathematical error is as old as mathematics itself.
Euclid's very first proposition asks, ``on a given straight line
to construct an equilateral triangle.''  Euclid's construction
makes the implicit assumption -- not justified by the axioms -- that
two circles, each passing through the other's center, must intersect.
We revere Euclid, not because he got everything right, but because
he set us on the right path.

We have entered an era of proofs of extraordinary complexity.
Take, for example, F. Almgren's masterpiece in geometric measure
theory, called appropriately enough the ``Big Paper.'' 
%http://www.worldscibooks.com/mathematics/4253.html
The preprint is
1728 pages long. Each line is a chore. He spent over a decade writing it in the 1970s and
early 80s.  It was not published until 2000.  Yet the theorem
is fundamental.  It establishes the regularity of minimizing
rectifiable currents, up to codimension two;  in basic terms, 
it shows that higher dimensional soap bubbles are smooth
rather than jagged -- just
as one would naturally expect.  How am I to develop enough confidence
in the proof that I am willing to cite it in my own research?
Do the stellar reputations of the author
and editors suffice, or should I try to understand the details of the
proof?  I
would consider myself very fortunate if I could work through the proof
in a year.

Computer proofs, which are sprouting up in many fields,
compound the complexity: 
the non-existence of a projective plane of order 10,
the proof that the Lorenz equations have a strange attractor,
the double-bubble problem for minimizing soap bubbles enclosing
two equal volumes, the optimality of the Leech lattice among
24-dimensional lattice packings, hyperbolic $3$-manifolds,
and the one that got it all started: the four-color theorem.
% W. Tucker solved Smale's 14th problem by computer, establishing
%that the Lorenz equations have a strange attractor. 
%A. Kumar and H. Cohn
%The Search for a Finite Projective Plane of Order 10
%http://www.cecm.sfu.ca/organics/papers/lam/paper/html/node5.html
%http://arxiv.org/abs/math/9609207 Homotopy 3-manifolds.
What assurance of correctness do complex computer proofs provide?



\section{Formal Proof}

Traditional mathematical proofs are written in a way to make them easily understood by mathematicians. Routine logical steps are omitted. An enormous amount of context is assumed on the part of the reader. Proofs, especially in topology and geometry, rely on intuitive arguments in situations where a trained mathematician would be capable of translating those intuitive arguments into a more rigorous argument.


A formal proof is a proof in which every logical inference has
been checked all the way back to the fundamental axioms of mathematics.
All the intermediate logical steps are supplied, without exception. No appeal is made to intuition, even if the translation from intuition to logic is routine. Thus, a formal proof is less intuitive, and yet less susceptible to logical errors.

There is a wide gulf that separates traditional proof from formal proof.
For example, Bourbaki's Theory of Sets was designed as a purely theoretical
edifice that was never intended to be used in the proof of actual theorems.
Indeed, Bourbaki declares that ``formalized mathematics cannot
in practice be written down in full'' and calls such a project
``absolutely unrealizable.''  % Bourbaki, Elements of Sets, Addison-Wesley, Reading, MA, 1968,  pp.10,11.
The basic trouble with various foundational systems is that meta-mathematical arguments (for
example, abbreviations that are external to the system or
inductions over the syntactical form of an expression) 
are usually introduced early on, and without these simplifying meta-arguments,
the vehicle stalls, never making it up the steep incline from primitive notions to 
high-level concepts.   The gulf can be extreme: A. Matthias has calculated
that to expand the definition of the number `$1$' fully in terms of Bourbaki primitives requires
over $4$ trillion symbols.
% A Term of Length 4,523,659,424,929 Synthese 133 (2002) 75--86.
In Bourbaki's view, 
the foundations of mathematics are roped-off museum pieces
to be silently appreciated, 
but not handled directly.

There is an opposing view that regards the 
foundational enterprise
as unfinished until it is realized in practice and written down in full.
This article sketches the current state of this endeavor.
It has been necessary to commence afresh, and to retool the foundations
of mathematics for practical efficiency, while preserving
its reliability and austere beauty.  For anything beyond a trivial
proof, the number of logical inferences is so large that a computer is
used to ensure that no steps are omitted.   This raises basic questions
about trust in computers.  This article also places formal proofs within
a broader context of automating more general mathematical tasks.

As the art is currently practiced, each formal proof starts with a traditional
mathematical proof, which is rewritten in a greatly expanded form, where all the
assumptions are made explicit and all cases are treated in full.
For example, a traditional mathematical proof might show that a graph is
planar by drawing the graph on a sheet of paper.  The expanded form of
the proof  replaces the picture by careful argument.  From the
expanded text, a computer script is prepared, which generates all
the logical inferences of the proof.  The transcription of a single traditional
proof into a formal proof is a major undertaking.

\bigskip
\noindent
\framebox{\parbox{4.8in}{
\smallskip
\centerline{\it Three Early Milestones}
\smallskip

1954 -- M. Davis programs the Presburger
algorithm for additive arithmetic into the
``Johniac'' computer at the Institute
for Advanced Study.  
Johniac proves that the sum of two
even numbers is even, to usher in the era of computer proof.

\smallskip

1956 -- The automation of Russell and Whitehead's
{\it Principia Mathematica\/} begins~\cite{NSS}.
By the end of 1959, Wang's procedure had generated proofs
of every theorem of the Principia in the predicate calculus~\cite{Wang}.

\smallskip

1968 -- N.G. de Bruijn designs
the first computer program to check the validity of general mathematical
proofs.  His program Automath eventually checked
every proposition in a primer that Landau had written for his
daughter on the construction
of real numbers as Dedekind cuts.  
% daughter: see http://www.cs.ru.nl/~freek/aut/aut-4.1-manual.pdf
% Landau as google book: http://books.google.com/books?id=U9B5FKvx3pYC
% Landau as pdf: http://www.cs.ru.nl/~freek/aut/
% The culminating result was the proof that $ii = - 1$.
\smallskip

}}
\bigskip

\bigskip
\noindent
\framebox{\parbox{4.8in}{
\smallskip
\centerline{\it N. G. de Bruijn}
\smallskip

On April 24, 2008, F. Wiedijk and I visited N. G. de Bruijn at his home
in  Nuenen, shortly before his
ninetieth birthday. (Nuenen is the Dutch town where 
Vincent van Gogh lived when he painted the {\it Potato Eaters}.)  We discussed
Automath, Brouwer, Heyting, and some of his coauthors
(Knuth and Erd\"os).  De Bruijn has contributed to many fields of mathematics, including
analytic number theory, Penrose tilings, quasicrystals, and optimal control.


\smallskip
De Bruijn indices give a notation that
eliminates all dummy  variables from formulas with
quantifiers: $\forall\,x.~P(x)$ becomes
$(\forall~P~1)$.  This notation solves the problem of
free variable capture.

\smallskip
De Bruijn observed that the ratio of lengths of a formal proof to
the corresponding conventional proof is remarkably
stable across different proofs.  The ratio, called the de Bruijn factor,
has become the standard benchmark to measure the overhead of a formal proof.



\smallskip
}}
\bigskip

\subsection{Examples}

Computer proof assistants have been under development for decades (see Box~\ref{XX}), % EDITOR: Early Milestones. 
but only recently
has it become a practical matter to prove major theorems formally.
The most spectacular
example is Gonthier's formal proof of the four-color theorem.  His
starting point is the second-generation 
proof by Robertson et al.  Although the traditional proof uses a computer
and Gonthier uses a computer,  the two computer processes
differ from one another in the same way that a traditional proof differs
from a formal proof.  They differ in the same way that adding $1+1=2$ on
a calculator differs from the mathematical
justification of $1+1=2$ by definitions,
recursion, and a rigorous construction of the natural numbers.  
In short, a large logical gulf separates them.
As a result of Gonthier's formalization,
the proof of the four-color theorem has become one of the most meticulously verified proofs
in history.

In recent years, several other significant theorems have been
formally verified. See Table~\ref{table}.  The table lists the
theorems, which proof assistant was used (there are many to choose from), the person who
produced a formal proof, and the mathematicians who produced the
original proof.  The Prime Number Theorem, asserting that the
number of primes less than $n$ is asymptotic to $n/\log\,n$, has
two essentially different proofs: the elementary proof of
Selberg and Erd\"os and the analytic proof of Hadamard and
de la Vall\'ee Poussin.  Formal versions of both proofs have
been produced.   More ambitious projects are in store:
Gonthier's team is now formalizing the Feit-Thompson odd order theorem, and
 the leading problem of the document {\it Ten Challenging
Research Problems for Computer Science} is the formalization
of the proof of Fermat's Last Theorem~\cite{Berg}.
% J. Bergstra, 5 July 2005.






\smallskip

\begin{table}[ht]
\caption{Examples of Formal Proofs}
\centering
\begin{tabular}{l l l l l}
\hline
Year\hspace{0.5em} &Theorem\hspace{8em} &Proof System\hspace{2em}  &Formalizer\hspace{3em} &Traditional Proof\\ [0.5ex]
\hline \\
1986 &First Incompleteness &Boyer-Moore   &Shankar &G\"odel \\
1990 &Quadratic Reciprocity&Boyer-Moore &Russinoff &Eisenstein\\
1996 &Fundamental - of Calculus &HOL Light &Harrison &Henstock\\
2000 &Fundamental - of Algebra &Mizar &Milewski    &Brynski\\ % email tchales@gamil.com, Aug 17 from Milewski mentions  Brynski.
2000 &Fundamental - of Algebra &Coq &Geuvers et al.   &Kneser\\
2004 &Four Color &Coq &Gonthier &Robertson et al.\\
2004 &Prime Number &Isabelle &Avigad et al. &Selberg-Erd\"os\\
2005 &Jordan Curve  &HOL Light &Hales &Thomassen \\
2005 &Brouwer Fixed Point &HOL Light &Harrison &Kuhn \\
2006 &Flyspeck I &Isabelle &Bauer-Nipkow &Hales \\
2007 &Cauchy Residue &HOL Light &Harrison &classical \\
2008 &Prime Number &HOL Light &Harrison &analytic proof \\
% 2008 &Flyspeck II &Isabelle &Obua &Hales \\
 [1ex]
\hline
\end{tabular}
\label{table}
\end{table}
% Shankar 1986 dissertation:
% 1986 asserted in Shankar's book, Metamathematics, Machines and Gödel's Proof (available as Google book, page xi.

% Quadratic Reciprocity: David M. Russinoff, A Mechanical Proof of Quadratic Reciprocity. J. Autom. Reasoning 8(1): 3-21 (1992).
% http://www.russinoff.com/papers/gauss.pdf
% 

% Fund. Th. of Alg: http://www.cs.ru.nl/~freek/100/
% Aug 21, 2000 for Mizar: http://mizar.uwb.edu.pl/JFM/pdf/polynom5.pdf
% Isabelle: 2008; http://isabelle.in.tum.de/library/HOL/HOL-Complex/Fundamental_Theorem_Algebra.html
% 
%
% A Constructive Proof of the Fundamental Theorem of Algebra without Using the Rationals
% See google book: http://books.google.com/books?id=wbHgOnNZNdYC
%
%Source	Lecture Notes In Computer Science; Vol. 2277 archive
%Selected papers from the International Workshop on Types for Proofs and Programs table of contents
%Pages: 96 - 111  
%Year of Publication: 2000
%ISBN:3-540-43287-6
%Authors	
%Herman Geuvers	
%Freek Wiedijk	
%Jan Zwanenburg	
%Publisher	
%Springer-Verlag  London, UK


Box~\ref{XX} % EDITOR : The Formal Jordan Curve Theorem
displays the statement of the Jordan Curve theorem, in computer
readable form, as it appears in the formal
proof.  The complete specification of the theorem should also list
all definitions, all the way back to the primitives.  Without giving the detailed definitions here, we note that {\it top2} refers
to the standard topology on the plane; {\it top2}~$A$ indicates that $A$ is an open set
in the plane;  $\hbox{\it euclid}\,\,2$ is the Euclidean plane; 
and {\it connected top2}~$A$ means that
$A$ is a connected set in the plane.

\bigskip
\noindent
\framebox{\parbox{4.8in}{
\parindent=0pt
\smallskip

\centerline{\it The Formal Jordan Curve Theorem}
\smallskip

\vbox{
\def\hb{\hfill\break}
\def\h{\hbox{}}
\def\s#1{\hskip#1}
\def\w{\hskip0.65em}

\obeylines

  {\tt %let JORDAN\_CURVE\_THEOREM = prove\_by\_refinement(
  \h~~~$\forall C.\s0.4em \hbox{\it simple\_closed\_curve}\s0.3em\hbox{\it top2} \s0.3em C\w\Rightarrow$\hb
  \h~~~~~( $\exists A\, B.\s0.4em\hbox  {\it top2}\s0.3em A\w\wedge\w{\it top2}\s0.3em B\w\wedge$\hb
  \h~~~~~~~$\hbox{\it connected}\s0.3em\hbox{\it top2}\s0.3em A\w\wedge\w\hbox{\it connected top2}\s0.3em B\w\wedge$\hb
  \h~~~~~~~$A \ne \emptyset\w\wedge\w B \ne \emptyset\w\wedge$\hb
  \h~~~~~~~$A \cap B = \emptyset\w\wedge\w A \cap C = \emptyset\w \wedge\w B \cap C = \emptyset\w \wedge$\hb
  \h~~~~~~~$A \cup B \cup C =\hbox{\it euclid}\hskip0.3em 2$ )
  \h~~~%$\cdots$);;


}
\smallskip
}


}}
\medskip




A large library is maintained
of all previously established proofs in the system, and anyone may
use any result that has been previously established.
 Although every step of every proof is
always checked,
as researchers contribute to the system,
interaction with the system gradually moves
away from the primitive foundations towards something more closely
resembling the high-level practice of mathematicians.
The hope
is they will eventually become sufficiently user-friendly
to become a familiar part of the 
mathematical workplace, much as email, \TeX\relax, computer algebra systems, and
web browsers are today.






\section{HOL Light}


This section gives a brief introduction to one foundational
system designed for doing mathematical proofs on a computer.
The system is called HOL Light
(an acronym for a lightweight implementation of Higher Order Logic).
I have singled it out because of its simple design and because it is
the system that I understand the best.
Some understanding of the design of a simple system is helpful before
turning to questions of soundness in the next section.
HOL Light by itself is only a small part of the overall formal-theorem-proving
landscape.
There are several competing systems to choose from, 
built on various logical foundations, and
 with their own powerful features.
People argue about the relative merits of the
different systems much in the same way that people argue about the relative merits
of operating systems, political loyalties, or programming languages.  To some extent, preferences
show a geographical bias:  HOL in the UK, Mizar in Poland, Coq in France, and Isabelle in Germany and the UK.


The basic components of the HOL Light system are its types, terms, theorems,
rules of inference, and
axioms.  Each is briefly described in turn.  Box~\ref{XX} % The HOL Light System
gives a summary
of the entire system.  

\subsection{Types}

Much day-to-day mathematics is written at a level of abstraction that is
indifferent to its exact representation  as sets.  For example, 
it does not matter how an ordered pair is encoded as a 
set, as long as the ordered pair has the characteristic property
 $$
 (x,y) = (x',y') \quad \Leftrightarrow\quad  x = x' \hbox{ and } y=y'.
 $$
It is bad style to break the abstraction to write $2\in(0,1)$.
This layer of abstraction is good news, because it allows us to shift
from Zermelo-Fraenkel-Choice (ZFC) set theory  to a different foundational system with equanimity and ease.  

Many proof assistants  are based on
types.  Types are familiar to computer programmers.  In a typed computer language,
$3$ is an integer and $[1.0;2.0;3.0]$ is an array of floating point numbers. An attempt
to add $3$ to this array results in a type mismatch error, and the computer program
will not compile.  The type checking mechanism of programming languages
conveniently detects many bugs at the time of compilation. 


ZFC set theory has no such type checking mechanism.  As de Bruijn
puts it,
``Theoretically, it seems perfectly legitimate
to ask whether the union of the cosine function
and the number $e$ (the basis of natural
logarithms) contains a finite geometry''~\cite{dbXY}.
%- N. G. de Bruijn, Types in Mathematics, page 29
Mathematicians have the good sense not to ask such questions.  However, when moving
mathematics to a computer, which is lacking in common sense, it is useful to introduce types
into the foundations
to prevent this kind of nonsense.  By convention,  a colon is written before the name of a type.  For instance, we write the type of the real number $e$ as $\tc\ring{R}$, or simply $e:\ring{R}$, to indicate that $e$ is a real number.
The cosine function has a different type $\tc\ring{R}\to\ring{R}$,
or $\cos:\ring{R}\to\ring{R}$.
The type of the union operator 
forces its two arguments to have the same type,
so that an attempt to take the union of the cosine function with $e$ is then flat out rejected.

HOL Light is a new axiomatic foundation with types, 
 different from the usual ZFC.
The types are presented in Box~\ref{XX}. % The HOL Light System.  
There are only two primitive
types, the boolean type {\it \tc bool} and an infinite type {\it \tc ind}. The rest are formed
with type variables joined by arrows.
A mechanism is also provided for creating a new type
that is in bijection with a nonempty subset of an existing type,
allowing the system
to be extended with types for ordered pairs, integers, rational numbers, real numbers,
and so forth.

\subsection{Terms}

Terms are the basic mathematical objects of the HOL Light system.  The syntax is based
on Church's $\lambda$-calculus, which uses the notation
   $$
   \lambda x.\ f (x)
   $$
to represent the function that takes $x$ to $f(x)$, what a mathematician would write
as $f:\ring{N}\to\ring{N}$, $x\mapsto f(x)$.  The name $\lambda$-calculus is derived
from the use of the letter $\lambda$ to mark function arguments.  Box~\ref{XX} % The HOL Light System
lays out
the construction of terms.



In ZFC set theory, there is a bijection of sets
  $$
  Z^{X \times Y} \simeq (Z^Y)^X.
  $$
In other words, a function $(x,y)\mapsto f(x,y)$
from the Cartesian product $X \times Y$ to $Z$ can be viewed as a function on
$X$ that maps $x$ to a function $f(x,\cdot):Y\to Z$.  The right-hand side of this
bijection is called the curried form of the function (named after the logician Haskell Curry).  
In typed systems, the curried form of multivariate functions is generally preferred.  
Treating $X,Y,Z$ as types,  we  write the type of the curried function as
$f:X\to (Y \to Z)$, or simply $f:X\to Y\to Z$.

The system has only two primitive constants.  One of them\footnote{The second constant
is the Hilbert choice operator $(\varepsilon)$, discussed below.
Recall that every term that is
not a variable, a function application, or $\lambda$-abstraction is a constant.  `Constancy' is thus a broader notion here than in first-order logic, and includes terms
such as equality that take arguments.  Parentheses are drawn around the
equality symbol $( = )$ to denote the prefixed curried form, with
$( = )\, x\, x$ an alternative syntax for $x = x$.} is
the equality symbol $( = )$
of type $\tc A\to A\to  bool$.  
That is, equality is a curried function that takes two arguments of the same type
and returns the boolean type.




\bigskip
\noindent
\framebox{\parbox{4.8in}{
\parindent=0pt
\smallskip
\centerline{\it The HOL Light System%\footnote{\tt Note to editor: typeset the entire HOL Light system on a single page.}
} % EDITOR
\smallskip

{\bf HOL Light} (Lightweight Higher Order Logic) is a foundational system designed for doing mathematical proofs on a computer.  The notation is based on a typed $\lambda$-calculus.  

\bigskip

{\bf 1. Types:}  The collection of types  is  freely generated from
{\it type variables} $\tc A, \tc B,\ldots$ and
{\it type constants}  $\tc bool$ (boolean), $\tc ind$ (infinite type),
joined by
{\it  arrows}  $( \to )$.   The colon is used as a notational device to indicate a type.
For example, $\tc bool$, $\tc bool\to A$, and $\tc(bool\to A)\to (ind \to B)$  are types. 

\bigskip
{\bf 2. Terms:}  The collection of terms is  freely generated from
{\it variables} $x,y,\ldots$ and {\it constants} $0,\ldots$ using
{\it abstraction} ($\lambda x. t$ where $x$ is a variable and $t$ a term) and {\it application} 
($f(x)$ for compatibly typed terms $x$ and $f$).  
Each term has a type.  The notation $x\tc A$ indicates that
the type of term $x$ is $\tc A$.  Variables and constants are assigned a type at the moment of creation; the types of abstractions and applications are defined recursively: the type of $\lambda x. t$ is $\tc A\to B$ when
$x\tc A$ and $t\tc B$; the type of $f(x)$ is $\tc B$ if $f\tc A\to B$ and $x\tc A$.

\bigskip
{\bf 3. Theorems:} A theorem is a {\it sequent} $\{p_1,\ldots,p_k\} \vdash q$,
where $p_1,\ldots,p_k,q$ are terms of type $\tc bool$.
The terms $p_1,\ldots,p_k$ are called the assumptions and $q$
is called the conclusion of the sequent.
The design of the system prevents the construction of theorems except through inferences from existing theorems, new definitions, and axioms.

}}

\bigskip
\noindent
\framebox{\parbox{4.8in}{
\parindent=0pt


\smallskip
{\bf 4. Inference Rules:}  The system has ten inference rules and a mechanism for defining new constants and types. Each inference rule is depicted as a fraction; the inputs to the rule are listed in the numerator, and the output in the denominator.  The inputs to the rules may be terms or other theorems.  In the following rules, we assume
that $p$ and $p'$ are equal, up to a renaming
of bound variables, and similarly for $b$ and $b'$.    
(Such terms are called $\alpha$-equivalent.)

\quad On first reading, ignore the assumption lists $\Gamma$ and $\Delta$. They propagate silently through the inference rules, but are really not what the rules are about.  When taking the union $\Gamma\cup\Delta$, 
$\alpha$-equivalent assumptions should
be considered as equal.
\smallskip

%\protect\twocolumn

%\framebox{
%\vbox{
\smallskip

Equality is reflexive:%\footnote{\tt Note to editor:  Typeset rules in a double column format, four per column. } % EDITOR
$$
\frac{a}{\vdash a=a}
$$

Equality is transitive:
$$
\frac{\Gamma \vdash a=b;~~~\Delta\vdash b'=c}
{\Gamma\cup\Delta \vdash a=c}
$$

Equal functions applied to equals are equal:
$$
\frac{\Gamma\vdash f=g;~~~\Delta\vdash a=b}
{\Gamma\cup\Delta\vdash f\hskip0.1em a = g\hskip0.1em b}
$$

The rule of abstraction holds. Equal terms
give equal functions:
$$
\frac{x;~~~\Gamma\vdash a=b}
{\Gamma \vdash \lambda x.\ a~=\lambda x.\ b}
~\hbox{\ (if $x$ is not free in $\Gamma$)}
$$

The application of the function $x\mapsto a$ to $x$ gives $a$:
$$
\frac{(\lambda x.~a)\, x}
{\vdash (\lambda x.\ a)\, x = a}
$$

%}}

%\pagebreak

%\framebox{\vbox{


%%ASSUME
Assume $p$, then conclude $p$:
$$
\frac{p\tc bool}
{p \vdash p}
$$

An `equality-based' rule of modus ponens holds:
%% EQ_MP
$$
\frac{\Gamma\vdash p;~~~\Delta \vdash p'=q}
{\Gamma\cup \Delta \vdash q}
$$
% equivalent Harrison's but order reversed.

If the assumption $q$ gives conclusion $p$ and the assumption $p$ gives $q$, then they
are equivalent:
$$
\frac{\Gamma \vdash p;~~~\Delta\vdash q}
{(\Gamma\setminus q)\cup (\Delta\setminus p)
\vdash p=q}
$$

Type variable substitution holds.  If arbitrary types are substituted in parallel for type variables in a sequent, a theorem results.
Term variable substitution holds.  If arbitrary terms are substituted in parallel for term variables in a sequent, a theorem results.

%}}


%\protect\onecolumn
\bigskip
{\bf 5. Mathematical Axioms:} There are only three mathematical axioms.
$$\begin{array}{lll}
\hbox{Axiom of Extensionality:} &\quad\forall f.\hskip1em(\lambda x.\, f\, x) = f.\\
\hbox{Axiom of Infinity:} &\quad\exists f\tc ind\to ind.~~(\op{ONE\_ONE}\,f) \land \neg(\op{ONTO}\, f).\\
\hbox{Axiom of Choice:}&\quad  
\forall P\,x.\hskip1em P x \Rightarrow  P(\varepsilon P).\\
\end{array}
$$
Extensionality asserts that every function is determined by its input-output relation. Dedekind's axiom of infinity asserts the existence of a function that is one-to-one but not onto.  The Hilbert choice operator $\varepsilon$ applied to a predicate $P$ chooses a term that satisfies the predicate, provided the
predicate is satisfiable.



}} % FRAMEBOX
\bigskip



\subsection{Axioms, Inference, and Theorems}


There are three mathematical axioms: an axiom of extensionality that asserts
that a function is determined by the values that it takes on all inputs,
an axiom of infinity that asserts that the type $\tc ind$ is not finite, and an axiom of choice.
The system has ten rules of inference, as described in Box~\ref{XX}. % The HOL Light System
For example, the first two state that equality is reflexive and transitive.
The final two rules
of inference allow one to substitute new terms for the free
variables in a theorem and  allow one to substitute new types for the
type variables in a theorem.  Beyond these ten rules of inference are mechanisms for
defining new constants and  new types.
A theorem is expressed in {\it sequent} form; that is as a set 
of assumptions, followed by a conclusion.  


\subsection{Extending the Primitive System}

This primitive system 
lacks the customary logical operators.
There are no symbols for 
`and', `or', `not', and `implies.'  There are no universal or existential quantifiers.   The set membership operator is absent.  
It is remarkable none of this is needed to express the rules of inference.

Logical operators
are defined later.
For example, the boolean constant $T$ (true) can be defined as the conclusion of
any theorem that has no assumptions.  The most accessible yet jarringly iconoclast
theorem comes from the reflexive law applied to equality itself:
$$
\vdash ( = ) ( = ) ( = ).
$$
Each new definition becomes a theorem.
So then $\vdash T = ((=) (=) (=))$.  Conjunction $( \land )$
is roundaboutly defined as the
curried function
 that on boolean inputs $p$ and $q$
returns $(\lambda f.\ f\, p\, q) = (\lambda f.\ f\, T\, T)$; that is,
conjunction yields  true exactly when no curried function $f$ is able to
distinguish $(p,q)$ from
$(T,T)$.
The other logical operations are built with similar tricks.

The inference rules and axioms
become bits of data that are processed by other computer procedures.
For example, to give a formal proof that 
$$
%% CHECKED May 31, 2008; Sep2.
2682440^4 + 15365639^4 + 18796760^4 = 20615673^4
$$
a human is not required to type each primitive inference. 
An automated procedure takes any arithmetic
identity as input,  generates the inferences,
and produces the theorem as output.   A large number of such
small decision procedures have been programmed into the system to handle
routine tasks such as polynomial simplification, basic tautologies in logic, 
and decidable fragments of arithmetic.  
Procedures that automatically search for steps in a proof
are also programmed into the computer.  
New procedures may be contributed
by any user at any time to automate further tasks.
The design of the kernel of the system prevents a rogue
user from writing computer code that could compromise the soundness of the system.


All the basic theorems of mathematics up through the Fundamental Theorem of Calculus are proved from scratch on the user's laptop in about two minutes every time the system loads, so
that the casual user does not need to be concerned with the low-level details.
Basic facts of logic and elementary mathematics are simply there in the system
to be used as needed.


\section{Soundness}

HOL Light is both an axiomatic system for doing mathematics and a computer program that implements the system.
How trustworthy is it?

If the computer is set aside for a moment, 
and the axiomatic system alone analyzed, it is known to be
consistent relative to ZFC.  That is, an inconsistency in
the HOL Light system would imply the inconsistency of ZFC. 



\subsection{Computer Implementation}

{\narrower\it  

You've got to prove the theorem-proving program correct. You're in a regression aren't you?
--A. Robinson~\cite[p.288]{Mac}.
% page 288, Chapter 8, MacKenzie.

}

\smallskip

The more pressing question is the soundness and reliability of the computer program that implements the
logic.  An earlier section reported that a typical software program has approximately one
bug per 100 lines of computer code.  The most reliable software ever created, for example
mission-critical software written for the space shuttle,
has fewer than one bug per 10,000 lines of computer
code.  Various proof assistants vary widely in reliability, ranging
from some of the world's most carefully crafted code at the upper end, to
rubbish at the lower end.  I  confine my attention to the
upper-end.
% http://amartester.blogspot.com/2007/04/bugs-per-lines-of-code.html



The computer code that implements the axioms and rules of inference is referred to as the kernel of the system.
It  takes fewer than
500 lines of computer code to implement the kernel of HOL Light.  
(By contrast, a Linux distribution contains approximately
283 million lines of computer code.)
% http://en.wikipedia.org/wiki/Linux (Code size) 
% In a later study, the same analysis was performed for Debian GNU/Linux version 4.0.[55] This distribution contained over 283 million source lines of code.
A bug anywhere in the kernel of this system might have fatal consequences.  For example,
if one of the axioms  is incorrectly typed, it might lead to an inconsistent system.




Yes, it is a regress; but a rather manageable regress.  The kernel
is a tiny amount of computer code, but hundreds of thousands of lines of code  are
verified by the kernel.  Eventually, there may be many millions of lines that are
verified by this small kernel.  The same kernel verifies everything from
the prime number theorem to the correctness of hardware designs.

Since the kernel is so small, it can be checked on many different levels.  The
code has been written in a friendly programming style for the benefit
of a human audience.  The source code is available for public scrutiny.   Indeed, the code
has been studied by eminent logicians.  
By design, the mathematical system is spartan
and clean.  The computer code has these same attributes.   A powerful type-checking mechanism within the programming language prevents a user from creating a theorem by any means except through this small fixed kernel.  Through type-checking, soundness is ensured, even after
a large community of users contributes further theorems and computer code. 
I wish to see a poster\footnote{A T-shirt has already been made!} 
% T-shirt Wiedijk email April 16, 2008 tchales@gmail.com
of the lines of the kernel, to be taught in undergraduate courses, and published throughout the world, as
the bedrock of mathematics.  It is math commenced afresh
as executable code.

Experience from other top-tier theorem-proving systems has been that about three to five bugs have been found
in each system over a period of 15-20 years of use.  After decades of use on many different
systems, to my knowledge, only one proof
has ever had to be retracted as a result of bug in a theorem-proving system,
and this in a system that I do not rank in the top-tier:  in 1995
a heap overflow error led to the false claim that the theorem-prover REVEAL had solved the Robbins
conjecture. %% page 289, MacKenzie.
We can assert with utmost confidence that 
the error rates of top-tier theorem-proving systems are 
orders of magnitude lower than
error rates in the most prestigious mathematical journals.  Indeed, since a formal proof starts with
a traditional proof, then does strictly more checking even at the human level, 
it would be hard for the outcome to be otherwise.

As an extra check, J. Harrison gave what can almost be described as 
a formal proof in HOL Light of its own soundness~\cite{HaSelf}.   To get around the self-referential limitations
imposed by G\"odel, he gave two separate proofs.  In the first proof, a weakened
version of HOL Light is created, without the axiom of infinity.  The standard version is
used to give a formal proof of the soundness of the weakened version.  In the second proof, a strengthened
version of HOL Light is created, with an additional axiom giving a large cardinal.  The strengthened
version then proves the standard version sound.  These proofs go beyond traditional relative consistency
proofs in logic in two respects.  First of all, they are formal proofs, rather than conventional proofs.
Second, the proofs establish not only the soundness of the logic, but also the underlying soundness
of the computer code implementing the logic.\footnote{The soundness of the computer code is considered
relative to a semantic model of the underlying programming language.  This model may differ from
the real-world behavior of the programming language, a reminder that the task of verification
is never complete.}

\subsection{Export}

In the past few years, a number of
programs have been written to automatically translate a proof written in one system into
a proof in another system.  If a proof in one system is incorrect because of an underlying
flaw in the theorem-proving program itself, then the export to
a different system fails, and the underlying flaw is exposed.  (Except of course,
unless the second theorem-proving program also has a bug that is perfectly aligned with the
bug in the first system.  Since these systems are largely independently designed and implemented, 
the events of failure in different systems are
treated as nearly independent, so that the probability of a perfect
alignment of failures across $n$ systems, goes to zero roughly as $p^n$, where $p$ is the individual
failure rate.)

Consider what happens when the proof of the soundness of HOL Light is exported.
(This has not happened yet, but should happen soon.)
The exported proof is a formal proof within a second theorem-prover that the HOL Light logic and implementation are sound.  
It will soon be within reach for several systems to give proofs
of one another's soundness.  When this is achieved, the probability of a false certification
of a pseudo-proof is pushed an order of magnitude closer to zero.  With a computer -- indeed with any physical artefact, whether a codex, transistor, or a flash drive made of proteins from salt-marsh bacteria --
% bug proteins 
% http://www.getusb.info/50-terabyte-flash-drive-made-of-bug-protein/
% http://www.tomshardware.com/news/bacteria-drives-store-terabytes,3125.html
it is never a matter of achieving philosophical certainty.
It is a scientific knowledge of the regularity of nature and human technology, akin to the scientific evidence
that Planck's constant $\hbar$ lies reliably within its experimental range.
Technology can push the probability of a false certification
ever closer to zero: $10^{-6}$, $10^{-9}$, $10^{-12},\ldots$. The intent
is 
that one day a system will store a million proofs without so much as a misplaced
semicolon.

A bug in the compiler, operating system,
or underlying hardware has the potential to compromise a formal proof. 
To minimize such bugs, formal proofs can be made about the
correctness of the ambient
computational environment.
Indeed, verification of hardware design, 
compilers, and computer
languages has long been one of the principal aims of formal methods.
HOL itself was initially created for hardware verification.
As early as 1989, a simple computer system from high-level language down to microprocessor was ``formally specified and mechanically
verified''~\cite{BHMY}.
%% quoted in MacKenzie page 243, ref 77.
Today, the semantics of various
high-level programming languages have been defined with complete mathematical
rigor~\cite{Harper}.  
In recent work that is nothing short of spectacular, X. Leroy has
developed a formally verified compiler for the C programming
language~\cite{CC}.  
(When
the target of a formal verification is a piece of computer code, rather
than a standard mathematical text, the formalization checks that the
computer code conforms to a precise specification 
of the algorithm; certifying that the computer code is bug free.)


\section{Full Automation}

Formal proofs are part of a larger project
of automating all 
mechanizable mathematical tasks, from conjecture making to concept formation.  
This section touches on the problem of fully automated proofs
-- the discovery of proofs
entirely by computer without any human intervention.  The next section
briefly describes the ultimate challenge of producing an automated mathematician.
Progress has been gradual.
Fifty years ago, it was famously predicted that
within a decade ``a digital computer will discover and prove
an important new mathematical theorem.''
%  MacKenzie, page 89.  H. Simon and A. Newell 
This did not happen as scheduled.


Most success has been with
the development of algorithms to solve special classes
of problems.  The WZ algorithm  gives  automated proofs of identities
of hypergeometric sums.   Gr\"obner basis
methods solve ideal membership problems.
Wu's geometry algorithm proves theorems such as
Pappus' theorem
and Pascal's theorem on the ellipse.
% See Chou's article "Proving Elementary Geometry Theorems Using Wu's Algorithm" in *25 years.*
Tarski's algorithm  solves
problems that can be formulated in the first-order language of the real numbers.
The list of specialized algorithms is in fact enormous.

The most widely acclaimed example of a fully automated computer proof
is the solution of the Robbins conjecture in 1996.  
The conjecture asserts that an alternative definition is equivalent
to the usual definition of a Boolean algebra.
It is remarkable
because the solution does not involve any human assistance,
specialized algorithms, or software
designed with this particular problem in mind.
Just type the problem into W. McCune's general purpose
theorem prover {\it EQN}, hit return, and wait
eight days for the solution to appear~\cite{Mc1},\cite{Mc2}.

Yet the story is only a qualified success.  It has
remained almost an isolated example, rather than the first in a torrent
of results.  The conjecture itself has the rather special form of a
word problem in an
abstractly defined algebraic system -- a type of
problem particularly suited for computer search.
The proof that was found by
computer can be expressed as a short yet non-obvious sequence
of substitutions. (See box.) % EDITOR: Full Automation of the Robbins..


\bigskip
\noindent
\framebox{\parbox{4.8in}{
\smallskip
\centerline{\it Full Automation of the Robbins Conjecture}
\smallskip
Let $S$ be a nonempty set with an associative commutative binary operation $(x,y)\mapsto xy$ and a unary operation $x\mapsto[x]$ (which, for convenience, we write synonymously as $x\mapsto \bar x$).
The Robbins conjecture (in Winker form) asserts that the general Robbins identity
   $$
   [[ab][a\bar b]] = a
   $$
implies the existence of $c,d\in S$ such that $[cd]=\bar c$.  Here is the original proof that
EQN discovered, as reconstructed in~\cite{fit}.
\begin{proof}  A solution is $c=x^3u$, $d=x u$, where $u=[x\bar x]$ and $x$ is arbitrary.
Abbreviate $j=[cd]$,  $e=u[x^2]\bar c$.  Over the equality sign, 
a prime indicates a direct application of the Robbins identity; a superscript
indicates a substitution of the numbered line; no superscript indicates a rewriting of abbreviations $c,d,e,j,u$.
$$
\begin{array}{lll}
%zero
 0: [u [x^2]] &= [[x\bar x][xx]] =' x.\\
%two
 1: [x u [x u [x^2]\bar c]] &=' [   [[x u x^2] [x u [x^2]]]  [x u [x^2]\bar c]] = 
       [  [\bar c [x u [x^2]]]   [\bar c x u [x^2]] ] =' \bar c.\\ 
%four
 2: [u\bar c]&= [u[x^2 u x]]=^0 [u[x^2 u[u[x^2]]]] =' [ [[u x^2][u [x^2]]] [x^2 u [u [x^2]]] ] 
  \\&='
   [u [x^2]] =^0 x.\\ 
%seven:
 3: [j u]&= [[xcu]u]=' [[xcu][[uc][u\bar c]]] =^2 [[xcu][x[cu]]] =' x\\
%eight
 4: [x[x[x^2]u\bar c]] &=' [  [[x[u\bar c]][x u \bar c]] [x [x^2]u \bar c]] =^2 [ [[x^2][x u \bar c]] [[x^2] x u\bar c]] =' [x^2]\\
%ten:
5: [x\bar c] &=^1 [x [x u [x u [x^2]\bar c]]] =^0  [[u [x^2] ] [x u [x u [x^2]\bar c]]]  
  \\&= [[u [x^2]] [u x[ x e]]] =^4 [  [u[x[xe]]][u x[xe]] ] ='     u\\
%thirteen:
6: [j x]&=' [j[[xc][x\bar c]]] =^{5} [j[[xc]u]] =[[uxc][u[xc]]]=' u\\
[1ex]
%
7:  [cd]&= j =' [[j[x\bar c]][jx\bar c]] =^{5} [[ju][jx\bar c]] =^3 [x[j x \bar c]] 
  =^2[[\bar c u][\bar c j x]]
  \\& =^{6} [ [\bar c [jx]][\bar c j x]] =' \bar c.
 \end{array}
$$
\end{proof}
}} 
\bigskip


Overall, the level today of fully automated computer proof (lying outside
special purpose algorithms) remains that of undergraduate
homework exercises: a group in which every element has order two is necessarily abelian; Cantor's theorem asserting that a set is not in bijection with its powerset; 
%
%JSTOR: 2004 Annual Meeting of the Association for Symbolic Logic
%E-mail: cebrown(andrew. cmu. edu. The Theorem Proving System TPS can be ... TPS can prove automatically are: THM 1 5B: If some iterate of function f has a ...
%links.jstor.org/ sici?sici=1079-8986(200503)11%3A1%3C92%3A2AMOTA%3E2.0.CO%3B2-V - Similar pages
if some iterate of a function has a unique fixed point, then
the function has a fixed point; the base $e$ for natural logarithms is irrational~\cite{TPS},~\cite{Bee}. 
Because of current limitations, fully automated proof tools
generally serve to fill in  intermediate steps of a
larger formal proof.  They are not ready to take on the Riemann hypothesis.


\section{Automated Discovery}

What happens if one sets aside rigor, and lets a computer explore?
A groundbreaking project was D. Lenat's 1976 Stanford thesis.
His computer program AM (for Automated Mathematician) was
designed to discover new mathematical concepts.  When AM was set loose to explore in the wild, it discovered the concepts of natural number, addition, multiplication, prime numbers, Pythagorean triples, and even the fundamental theorem of arithmetic.
The thesis touched off a firestorm of criticism and praise.

To put AM in context, consider a hypothetical program that is instructed to discover new concepts by deleting conditions
from the list of axioms defining a finite abelian group.  The computer will then immediately discover the concepts
of infinite  group,  nonabelian group,  monoid, and so forth because these concepts all
arise as subsets of the axioms.  These discoveries could be sensationalized:
{\it A program in Artificial Intelligence 
has made the ultimate leap from the finite to infinite, and from the abelian to the nonabelian,
rediscovering fundamental concepts in seconds that mathematicians have grappled with for centuries.}
There are nagging questions about the emptiness of AM's discoveries; a
suggestive representation of the problem gives the answer away.

More recent projects stir the imagination, even if the field
is still young.
Computer programs have generated over one thousand
 conjectures in graph theory, expressing numerical relationships
between different graph invariants.
One open conjecture is described in Box~\ref{XX}. % An Open Computer-Generated Conjecture
No technological barriers prevent us from unleashing conjecturing
machines in all branches of mathematics,
to see what  moonshine they reveal.


\bigskip
\noindent
\framebox{\parbox{4.8in}{
\smallskip
\centerline{\it An Open Computer-Generated Conjecture}
\smallskip
Let $G$ be a finite graph with the following properties:
  \begin{enumerate}
  \item It has at least two vertices.
  \item The graph is simple; that is, there are no loops or multiple joins.
  \item It is regular; that is, every vertex has the same degree.
  \item The graph is connected.
  \end{enumerate}
For example, the complete graph (the graph with an edge between every two vertices)  on $n$ vertices has these properties, when $n\ge 2$.
Define the {\it total domination number} of $G$ to be the size of the smallest
subset of vertices such that every vertex of $G$ is adjacent to some vertex in the
subset.    The {\it path covering number} is the size of the smallest partition of the vertices
into subsets, such that there exists a path confined to each subset $S$ that steps through
each vertex of S exactly once (that is, the induced graph on $S$ has a Hamiltonian path).  
\smallskip

The computer program Graffiti.pc conjectures that {\it the total domination number of $G$ is at least
twice the path covering number of $G$}.
For example, the complete graph on $n$ vertices has path covering number one,
because it has a Hamiltonian path.  Its total domination number is two (take any two vertices).
The conjecture is sharp in this case by these direct observations~\cite{DLPWW}.

\smallskip
}} 


\section{Flyspeck}

My interest in formal proofs grows out of a practical desire for
a thorough verification of my own  research that goes beyond what
the traditional peer review process has been able to provide.
A few years ago, I launched a project called {\it Flyspeck} to
give a formal proof of the Kepler conjecture, asserting that
no packing of congruent balls in three-dimensional Euclidean space
can have density greater than the density of the face-centered cubic
packing (also known as the cannonball arrangement).  The name
Flyspeck, which quite appropriately can mean to scrutinize, is derived from the acronym FPK, for the Formal Proof of the Kepler
conjecture.  


The original proof of this theorem was unusually difficult
to check.  In a letter of qualified acceptance 
for  publication in the {\it Annals of Mathematics}, an editor described the process, ``The referees put a level of energy into this that is, in my experience, unprecedented. They ran a seminar on it for a long time. A number of people were involved, and they worked hard. They checked many local statements in the proof, and each time they found that what you claimed was in fact correct. Some of these local checks were highly non-obvious at first, and required weeks to see that they worked out\ldots
They have not been able to certify the correctness of the proof, 
and will not be able to certify it in the future, because they 
have run out of energy to devote to the problem.''  In addition to
a $300$ page text, the proof relies
on about forty thousand lines of custom computer code.  To the best of my knowledge,
the computer code was
never carefully examined by the referees.
The policy of the {\it Annals of Mathematics}
states, ``The human part of the proof, which reduces the original mathematical problem to one tractable by the computer, will be refereed for correctness in the traditional manner. The computer part may not be checked line-by-line, but will be examined for the methods by which the authors have eliminated or minimized possible sources of error$\ldots$''

Ultimately, the mathematical corpus is no more reliable than the processes
that assure its quality.  A formal proof attains a much
higher level of quality control than can be achieved by ``local checks''
and an ``examination of methods.''


Flyspeck may take as many as twenty work-years to complete. S. Obua and G. Bauer have already defended Ph.D. theses 
on the  project.  
Together with the work of
their advisor T. Nipkow (one of the principal architects of the Isabelle proof assistant), nearly half of the computer code
used in the proof of the Kepler conjecture is now 
certified.

\section{QED}


The Flyspeck project is minute speck in the overarching Q.E.D. project
(an anonymous manifesto declaring that all significant mathematical results should be preserved in a vast library of formal proofs).
The labor required to realize such a library would be staggering.
In the Notices in 1991, de Bruijn proposed an assembly line
to turn mathematical ideas into formally verified proofs~\cite{dB91}.
The standard benchmark for the
human labor to transcribe one printed page
of textbook mathematics into machine verified formal text is one week, or~\$150 per page at an outsourced wage. To undertake the formalization of just $100,000$ pages
of core mathematics 
would be one of the most ambitious collaborative projects ever undertaken in pure mathematics, the sequencing of a mathematical genome. 
One might imagine a massive wiki collaboration that
 settles  the 
text of the most significant theorems in contemporary mathematics from 
Poincar\'e to Sato-Tate.

Outsourcing is the brute force solution to the Q.E.D. manifesto.  Most researchers, however, prefer beauty over brute force;
we may hope for advances in our 
understanding that will permit us someday to convert a printed page of textbook mathematics into machine verified formal text in a matter of hours, rather than after a full week's labor.  As long as
transcription from traditional
proof into formal proof is based on human labor rather than
automation, formalization remains an art rather than a science.
Until that day of automation, 
we fall short of a practical understanding of the foundations of mathematics.



\section{Recommended Reading and Software}

By far the best overview of the subject is the book {\it Mechanizing Proof,}  winner of the 2003 Merton Book Award of
the American Sociological Association~\cite{Mac}.
% Review by Hayes: http://www.americanscientist.org/template/BookReviewTypeDetail/assetid/12866.
The Q.E.D. Manifesto can be found at~\cite{QED}.
Historical surveys include~\cite{Bled},
%``Automated Theorem Proving after 25 Years,'' 
\cite{Ha07},~\cite{Gor}, and
% A short survey of automated reasoning.
\cite{Mu}.
% Present State of Mechanical Deduction
For something more comprehensive, see~\cite{Ha09}.
% Harrison's book


Several theorem proving systems are extensively documented and are available for download,
including HOL Light~\cite{HOLL}, Isabelle~\cite{Isa}, Coq~\cite{COQ},
 Mizar~\cite{Mizar}, TPS, 
PVS,
ACL2, 
NuPRL, and MetaPRL.
A web-browser version
of Coq allows one to experiment with a proof assistant without
downloading any software~\cite{PW}.


\begin{thebibliography}{A}



\bibitem{TPS} P. B. Andrews and C. Brown, ``Proving theorems and teaching logic with 
TPS and ETPS,'' The Bulletin of Symbolic Logic 
Volume 11, Number 1, March 2005.
% http://www.math.ucla.edu/~asl/bsl/1101/1101-005.ps

\bibitem{Bee} Michael Beeson. ``Automatic generation of a proof of the irrationality of $e$'' Journal of Symbolic Computation, 
32(4):333-349, 2001.

\bibitem{Berg} J. Bergstra, Nationale Onderzoeksagenda Informatie en Communicatietechnologie (NOAG-ict) 2005--2010, Albani drukkers, Den Haag, 2005.

\bibitem{BHMY} W. R. Bevier, W. A. Hunt, Jr. J Strother Moore, W. D. Young, ``An approach to Systems Verification''  Journal of Automated Reasoning 5 (1989); 411-428, at pp. 422-423.

\bibitem{Bled} W. W. Bledsoe and D. W. Loveland (eds.), Automated
Theorem Proving: After 25 Years, Contemporary Mathematics, Vol. 29,
AMS, Providence, RI, 1984.

\bibitem{dbXY} N. G. de Bruijn, On the Role of Types in Mathematics,
http://www.win.tue.nl/~wsdwnb/, 
1995.

\bibitem{dB91}  N.G. de Bruijn, Checking Mathematics with Computer Assistance, Notices of the AMS,
Vol 38, (1), Jan. 1991.

\bibitem{COQ} The Coq proof assistant, http://coq.inria.fr/.

\bibitem{DLPWW} E. DeLaVina, Q. Liu, R. Pepper, B. Waller and D. B. West, On some conjectures of
Graffiti.pc on total domination, Congressus Numerantium, (2007), Vol. 185, 81-95.

\bibitem{fit}  B. Fitelson, ``Using Mathematica to Understand the Computer Proof of the Robbins Conjecture''
Mathematica in Education and Research (Winter 1998, Volume 7, No. 1).

\bibitem{Gor} M. Gordon, From LCF to HOL: a short history,
Proof, language, and interaction: essays in honour of Robin Milner,
169 - 185, MIT, 2000.

\bibitem{HOLL} J. Harrison, The HOL Light theorem prover,
http://www.cl.cam.ac.uk/\~~jrh13/hol-light/index.html

\bibitem{Ha07} J. Harrison, A short survey of automated reasoning,
Proceedings of AB 2007, the second international conference on Algebraic Biology, Springer LNCS vol. 4545, pp. x--x, 2007.

\bibitem{Ha09} J. Harrison, Handbook of Practical Logic and Automated Reasoning, 704pp. 2009.

\bibitem{HaSelf} J. Harrison, Towards self-verification of HOL Light.

\bibitem{Isa} Isabelle, http://isabelle.in.tum.de/.

\bibitem{KFN} C. Kaner, J. Falk, H. Nguyen, Testing Computer
Software, Wiley, 1999.  

\bibitem{KBP} Kaner, Bach, Pettichord,
Lessons Learned in Software Testing 2001.

\bibitem{CC} % X. Leroy et al.  Compcert, http://compcert.inria.fr/.
X. Leroy. Formal certification of a compiler back-end, or:
programming a compiler with a proof assistant. In 33rd ACM symposium
on Principles of Programming Languages, pages 42-54. ACM Press, 2006.

\bibitem{Mac} D. MacKenzie, Mechanizing Proof, MIT Press, Cambridge, MA,
2001.

\bibitem{Mc1} W. McCune, Robbins Algebras are Boolean, http://www.cs.unm.edu/\~~mccune/papers/robbins/

\bibitem{Mc2} W. McCune, Solution of the Robbins Problem, JAR 19(3), 263--276 (1997)

\bibitem{Harper} R. Milner, M. Tofte, R. Harper, The Definition of Standard ML, MIT Press, 1990.

\bibitem{Mizar} Mizar Home Page, http://mizar.org/.

\bibitem{Mu} R. Murawski, The Present State of Mechanized
Deduction, and the Present Knowledge of Limitations,
Studies in Logic, Grammar and Rhetoric 
year: 2006, vol: 9, number: 22, pages: 31-60.

\bibitem{NSS} A. Newell, , J. C. Shaw and H. A. Simon, 1956, Empirical Explorations of the Logic Theory Machine: A Case Study in Heuristics. In: Proc. Western Joint Computer Conf., 15, pp. 218-239. Also in: Feigenbaum and Feldman (Eds.), Computers and Thought. McGrow-Hill 1963.

\bibitem{NYT} Adding Math to List of Security Threats, New York Times, November 17, 2007.

\bibitem{PW} ProofWeb, http://prover.cs.ru.nl/login.php.


\bibitem{QED} The QED Manifesto, ``Automated Deduction - CADE 12'', Springer-Verlag, Lecture Notes in Artificial Intelligence, Vol. 814, pp. 238-251, 1994.
http://www.cs.ru.nl/\~~freek/qed/qed.html.

\bibitem{Wang} H. Wang, Computer Theorem Proving and Artificial Intelligence,
 in~\cite{Bled}, 49--70.

\end{thebibliography}


\end{document}

